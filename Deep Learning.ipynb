{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filepaths = glob.glob('dataset-original/*')\n",
    "classes = [] # list of files for each class\n",
    "category_mapping = {} # map category name to integer\n",
    "\n",
    "for i in range(len(image_filepaths)):\n",
    "    classes.append(glob.glob(image_filepaths[i] + '/*.jpg'))\n",
    "    category_mapping[i] = image_filepaths[i].split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'paper', 1: 'metal', 2: 'cardboard', 3: 'trash', 4: 'glass', 5: 'plastic'}\n"
     ]
    }
   ],
   "source": [
    "print(category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6 \n",
      "\n",
      "paper : 594 images\n",
      "metal : 410 images\n",
      "cardboard : 403 images\n",
      "trash : 137 images\n",
      "glass : 501 images\n",
      "plastic : 482 images\n",
      "\n",
      "Total number of images 2527\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes:\",len(classes),'\\n')\n",
    "\n",
    "for idx,ic in enumerate(image_filepaths):\n",
    "    print(ic.split('/')[1],\":\",len(classes[idx]),\"images\")\n",
    "\n",
    "NUM_IMAGES = sum(len(c) for c in classes)\n",
    "print(\"\\nTotal number of images\",NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT,WIDTH = 150,150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2527, 150, 150, 3) (2527, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((NUM_IMAGES,HEIGHT,WIDTH,3))\n",
    "y = np.zeros((NUM_IMAGES,1))\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT! ONLY LOADING 10 IMAGES FROM EACH CLASS NOW, PLS CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "images_per_class = 10 # TODO: CHANGE HERE\n",
    "\n",
    "for idx,c in enumerate(classes): \n",
    "    for image in c[:images_per_class]: \n",
    "        im = Image.open(image).resize((HEIGHT,WIDTH))     \n",
    "        X[k] = im\n",
    "        y[k] = idx\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_classes = 6\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "total_images = num_classes * images_per_class\n",
    "# the data, split between train and test sets\n",
    "## TODO: CHANGE 6*10 here too to - (6* num images per category)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:total_images], y[:total_images], test_size=0.20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 150, 150, 3) (12, 150, 150, 3) (48, 1) (12, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (48, 150, 150, 3)\n",
      "48 training samples\n",
      "12 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'training samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices (one-hot representation)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1) (12, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out data augmentation - can also be done when reading data from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train)/batch_size, \n",
    "                    epochs=epochs, verbose = 1,\n",
    "                    validation_data = test_datagen.flow(X_test, y_test),\n",
    "                    validation_steps = len(X_test)/batch_size)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
